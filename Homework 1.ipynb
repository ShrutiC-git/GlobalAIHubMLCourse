{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. How would you define Machine Learning?\n",
    "\n",
    "In a typical program, a developer defines a set of given rules and feed it into the machine to get an answer. Example, a function to calcualte the square of a number: a developer explicitly defines a function (or define rules) to calculate this square. Once the fucntion is defined, the machine uses it to give an output.\n",
    "\n",
    "However, in Machine Learning, a machine is not explicitly programmed or given rules to find answers to a problem. Instead, it \"learns\" over time what a problem is and what solution it generates. For example, to predict if an email is spam or not, a machine learning model analyses past spam emails, without the programmer having to explicitly define what categorizes a spam email. When the machine has learned, it gains the capability of smartly making decisions based on some input features. Machine Learning is when a machine performs a task without being given direct instructions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. What are the differences between Supervised and Unsupervised Learning? Specify example 3 algorithms for each of these.\n",
    "\n",
    "Supervised learning is when a machine is being trained using the right answers to a problem which will allow the machine to give or choose the correct answer based on what it has learned previously. A machine is trained with data which has independent variables and a dependent variable, and the machine learns the relation between the variables. Once the machine learns this past behavior, it then uses its learning to predict a new value for data it has not seen before. It is similar to when a student is given answers to a bunch of addition problems until the student understands the concept behind addition and then can perform addition on numbers different that what they used to learn addition. Here the machine is given answers to questions and learns how to get to answers for those questions.\n",
    "Examples: \n",
    "1. Categorising emails as spam or not based on features like subject, sender etc\n",
    "2. Finding the average income of a person based on age, education etc\n",
    "3. Predicting the likelohood of a disease based on BMI, other conditions etc\n",
    "\n",
    "\n",
    "Unsupervised learning is when there are no answers, right or wrong, given to the machine. In this case, the machine is given data, with no refence to which variables are independent or dependent. The goal here is to understand the relation between different features of data. Going back to the analogy of a student, in unsupervised learning, a student would just be given a set of numbers with no real answers, and the student will have figure out whatever relation they can between different numbers. Unsupervised learning becomes important when we are trying to understand the relation bw data. Maybe the machine understands a relation which was not apparent to developers at all. \n",
    "Examples:\n",
    "1. Grouping users of services to understand the demographics of customers\n",
    "2. Drug-Target interaction problems to undertsand the relation between drug and target groups\n",
    "3. Serve personalized content to similar groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. What are the test and validation set, and why would you want to use them?\n",
    "\n",
    "Test set is the data which is not fed into the model during training. Once the model is trained and validated, this set is used to predict how well is our model performing on data it has not seen before. It is important to use this test set becuase often when we train the model with training data, there might be some variation in the training data which the model learns, and this variation might not occur again, and thus, testing our model using a varied data set is essential to understand the performance of our model generally.\n",
    "\n",
    "Validation set is the data set which is part of the training set but is used to find and optimize a model better for using it for real-world or unseen data. This set helps developers choose the best settings for our model and further make our model better. This is part of the training set but is not used in the learning process. Once the model has learned, validation set comes in to tune the model and make it even better. Then the model can be tested to see how the entire model performs on unseen data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. What are the main preprocessing steps? Explain them in detail. Why we need to prepare our data?\n",
    "We need to prepare our data becuase data that is acquired in real-time like from sensors or sales data, is generally unclean and do not tell the whole story correctly. For example, there might be data which is duplicated several times and is skewing the results. Or consider an example of analysing mean income of people in a country to make tax decisions. This model might be skewed because a lot of people do not report their income or there might be extreme lows or highs which can affect making proper decisions for everyone in the country. It is therefore important to prepare and process our data. \n",
    "\n",
    "Main Steps for preprocessing the data once it is acquired are:\n",
    "1. Checking for null or missing values, and thinking about how to replace them. Dropping these values might imbalance our data, so we need to think of ways of replacing the data(or if we have to drop then how does it affect our model).\n",
    "2. Checking and dropping duplicate values, because it skewes the representation of our data.\n",
    "3. Detecting and dealing with outliers. For example, going back to the example of financial polciies for countries, there might be outliers with exceptionally high income, and they are not representative of mean income of the country. While using this data, it skews our mean and represnetation. So we have to be mindful of outliers and see ways of dealing with them.\n",
    "4. Dealing with categorical values. Where nominal values can be changed into numerical values using one-hot encoded and ordinal values can be represented simply with numbers. This is done to make it easier to use these features and labels in our model. \n",
    "5. Normalizing data. This is specially crucial when we are trying to understand the importance of features. Features like income varies in scale with age, and will make our model think that income is a more relevant feature. To prevent this, and make features comparable, we use normalization so our features range from 0 to 1.\n",
    "6. Standardise the data. We standardise the data to represent it as a normal distribution with a mean of 0 and a standard deviation of 1. This is also done to make sure that our features are comparable. However, often times, when normalizing data, our model does not represnet outliers. If we need to depend upon outliers and take into consideration their values, standardisation is more accurate. \n",
    "7. Once all the above steps are done, we can split our data into train, test (and validation) to feed into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. How you can explore countionus and discrete variables?\n",
    "Discrete variables can be explored using bar chart. Each discrete value represents a bar, and its frequency is given by the height of the bar on the y-axis.\n",
    "\n",
    "Continuous variables can be explored using histogram. To better understand distribution, a histogram uses bins to group a range of continuous data, and plot the frenquency of the group (or bin) through the height of the histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Analyse the plot given below. (What is the plot and variable type, check the distribution and make comment about how you can preproccess it.)\n",
    "\n",
    "The plot is a histogram with 2 apparent data peaks. \n",
    "The variable is a continuous variable.\n",
    "It seems like we have some information missing about petal width. We will first have to analyse if no rows exist for this entry, or if there are no entries for the width while we have info about other features for this entry. Depending upon that, we can either drop of fill null value with the mean. \n",
    "Also, in order to scale the feature and make it comparable across other features, we can apply normalisation, which would ensure our petal width ranges between 0 and 1, and also make it easier to work with other features if there are any. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
